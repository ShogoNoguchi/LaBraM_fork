{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNki4Dn4Py6Tv8nhawHLvnX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShogoNoguchi/LaBraM_fork/blob/main/%EF%BC%88%E9%80%B2%E6%8D%97%EF%BC%89%EF%BD%9E%E5%A4%A7%E8%A6%8F%E6%A8%A1%E6%A7%8B%E9%80%A0%E3%81%AE%E7%89%B9%E5%AE%9A%E3%81%A8%E4%BF%AE%E6%AD%A3%E9%A0%85%E7%9B%AE%E3%81%AE%E7%99%BA%E8%A6%8B%EF%BD%9E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "文責：群馬大学　野口翔伍\n",
        "\n",
        "題：（進捗）～大規模構造の特定と修正項目の発見～"
      ],
      "metadata": {
        "id": "zB1PzacrXSHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Driveをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR96Dz5cSUoF",
        "outputId": "c10cd01c-4d8c-42c7-9de2-d2216585813e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "論文を熟読し、GitHubのREAD MEを読んで課題1つ目を処理する方法を調べた。\n",
        "TUAB・TUEVどちらのデータセットを使っても良いと思ったが、どちらもTemple Universityへの要求方法が不明なため、質問済。\n",
        "➡のちに、これらは使わず、SEED-Vを用いた感情分類をすることが判明。"
      ],
      "metadata": {
        "id": "y9B--gvwROlz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "リポジトリを[/content/drive/My Drive/LaBraM]へダウンロード済 12/2"
      ],
      "metadata": {
        "id": "xhc-OS48JQwd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-Gv59BL-o81",
        "outputId": "39f4926a-388d-45f7-d741-ba38c0e8f4b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content\n",
            "Cloning into 'labram_repo'...\n",
            "remote: Enumerating objects: 124, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 124 (delta 49), reused 45 (delta 42), pack-reused 57 (from 1)\u001b[K\n",
            "Receiving objects: 100% (124/124), 167.16 MiB | 26.39 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n",
            "Updating files: 100% (27/27), done.\n",
            "ディレクトリ構造:\n",
            "LaBraM/\n",
            "  checkpoints/\n",
            "    labram-base.pth/\n",
            "    vqnsp.pth/\n",
            "  data_processor/\n",
            "    data_preprocess.py/\n",
            "    dataset.py/\n",
            "  dataset_maker/\n",
            "    make_TUAB.py/\n",
            "    make_TUEV.py/\n",
            "    make_h5dataset_for_pretrain.py/\n",
            "    shock/\n",
            "      __init__.py/\n",
            "      utils/\n",
            "        __init__.py/\n",
            "        eegUtils.py/\n",
            "        h5.py/\n",
            "        ringBuffer.py/\n",
            "  engine_for_finetuning.py/\n",
            "  engine_for_pretraining.py/\n",
            "  engine_for_vqnsp.py/\n",
            "  labram.png/\n",
            "  modeling_finetune.py/\n",
            "  modeling_pretrain.py/\n",
            "  modeling_vqnsp.py/\n",
            "  norm_ema_quantizer.py/\n",
            "  optim_factory.py/\n",
            "  README.md/\n",
            "  requirements.txt/\n",
            "  run_class_finetuning.py/\n",
            "  run_labram_pretraining.py/\n",
            "  run_vqnsp_training.py/\n",
            "  utils.py/\n",
            "\n",
            "[checkpoints]フォルダの内容:\n",
            "  labram-base.pth\n",
            "  vqnsp.pth\n"
          ]
        }
      ],
      "source": [
        "# Google Driveをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 保存先ディレクトリの設定\n",
        "import os\n",
        "drive_path = '/content/drive/My Drive/LaBraM'  # Drive内に保存するフォルダ名\n",
        "if not os.path.exists(drive_path):\n",
        "    os.makedirs(drive_path)\n",
        "\n",
        "# リポジトリをクローン\n",
        "repo_url = \"https://github.com/935963004/LaBraM.git\"  # GitHubのURL\n",
        "%cd /content\n",
        "!git clone {repo_url} labram_repo\n",
        "!cp -r labram_repo/* \"{drive_path}\"\n",
        "\n",
        "# ディレクトリ構造を表示\n",
        "import os\n",
        "\n",
        "def display_directory_structure(path, level=0):\n",
        "    indent = \"  \" * level\n",
        "    print(f\"{indent}{os.path.basename(path)}/\")\n",
        "    if os.path.isdir(path):\n",
        "        for item in os.listdir(path):\n",
        "            display_directory_structure(os.path.join(path, item), level + 1)\n",
        "\n",
        "print(\"ディレクトリ構造:\")\n",
        "display_directory_structure(drive_path)\n",
        "\n",
        "# 推奨ファイルを特定するために 'checkpoints' フォルダの内容を表示\n",
        "checkpoints_dir = os.path.join(drive_path, 'checkpoints')\n",
        "if os.path.exists(checkpoints_dir):\n",
        "    print(\"\\n[checkpoints]フォルダの内容:\")\n",
        "    for file in os.listdir(checkpoints_dir):\n",
        "        print(f\"  {file}\")\n",
        "else:\n",
        "    print(\"\\n[checkpoints]フォルダが存在しません。\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "大規模コードの木構造を特定 12/3"
      ],
      "metadata": {
        "id": "SfaPrArmER3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ファインチューニング時にはGiTHubの以下のコマンドにより\n",
        "run_class_finetuning.pyが最初に起動される。"
      ],
      "metadata": {
        "id": "GGdteki6Kw8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OMP_NUM_THREADS=1 torchrun --nnodes=1 --nproc_per_node=8 run_class_finetuning.py \\\n",
        "        --output_dir ./checkpoints/finetune_tuab_base/ \\\n",
        "        --log_dir ./log/finetune_tuab_base \\\n",
        "        --model labram_base_patch200_200 \\\n",
        "        --finetune ./checkpoints/labram-base.pth \\    #baseモデルの重み\n",
        "        --weight_decay 0.05 \\\n",
        "        --batch_size 64 \\\n",
        "        --lr 5e-4 \\\n",
        "        --update_freq 1 \\\n",
        "        --warmup_epochs 3 \\\n",
        "        --epochs 30 \\\n",
        "        --layer_decay 0.65 \\\n",
        "        --drop_path 0.1 \\\n",
        "        --dist_eval \\\n",
        "        --save_ckpt_freq 5 \\\n",
        "        --disable_rel_pos_bias \\\n",
        "        --abs_pos_emb \\\n",
        "        --dataset TUAB \\   #データセット名の指定\n",
        "        --disable_qkv_bias \\\n",
        "        --seed 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "6GFLZizLMgK9",
        "outputId": "06631800-17d3-4cdd-baec-26533e9e317a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-ce411afbb617>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-ce411afbb617>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    OMP_NUM_THREADS=1 torchrun --nnodes=1 --nproc_per_node=8 run_class_finetuning.py \\\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "run_class_finetuning.pyから、以下の依存関係を特定した。"
      ],
      "metadata": {
        "id": "i24SlkJsMivZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_class_finetuning.py\n",
        "├── argparse (標準ライブラリ)\n",
        "├── datetime (標準ライブラリ)\n",
        "├── numpy (外部ライブラリ)\n",
        "├── time (標準ライブラリ)\n",
        "├── torch (外部ライブラリ)\n",
        "│   ├── torch.backends.cudnn\n",
        "│   ├── torch.utils.data\n",
        "│   └── torch.nn\n",
        "├── json (標準ライブラリ)\n",
        "├── os (標準ライブラリ)\n",
        "├── pathlib.Path (標準ライブラリ)\n",
        "├── collections.OrderedDict (標準ライブラリ)\n",
        "├── timm (外部ライブラリ)\n",
        "│   ├── timm.data.mixup.Mixup\n",
        "│   ├── timm.models.create_model   #モデル構造を入れて、モデルを出力\n",
        "│   ├── timm.loss.LabelSmoothingCrossEntropy\n",
        "│   ├── timm.loss.SoftTargetCrossEntropy\n",
        "│   └── timm.utils.ModelEma\n",
        "├── optim_factory.py     #最適化関連\n",
        "│   ├── create_optimizer\n",
        "│   ├── get_parameter_groups\n",
        "│   └── LayerDecayValueAssigner\n",
        "├── engine_for_finetuning.py  #ファインチューニング用のエンジン\n",
        "│   ├── train_one_epoch\n",
        "│   └── evaluate\n",
        "├── utils.py\n",
        "│   ├── NativeScalerWithGradNormCount   #学習率スケーリング\n",
        "│   ├── init_distributed_mode\n",
        "│   ├── create_ds_config\n",
        "│   ├── get_rank\n",
        "│   ├── cosine_scheduler\n",
        "│   ├── auto_load_model\n",
        "│   ├── save_model\n",
        "│   ├── is_main_process\n",
        "│   ├── prepare_TUAB_dataset   #ここを変える必要がある。 utils内のprepare_oo_dataset系\n",
        "│   ├── prepare_TUEV_dataset\n",
        "│   ├── TensorboardLogger\n",
        "│   └── load_state_dict\n",
        "├── modeling_finetune.py    #ここを調べるとファインチューニングのモデル構造が分かる・変更できる。\n",
        "│   └── モデルの定義\n",
        "├── scipy.interpolate (外部ライブラリ)  #これはなんでインポートされてるか分からない...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "VVOXJZstMu0J",
        "outputId": "85b669f3-9ca8-4aff-fa55-af4cf514cd4c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character '├' (U+251C) (<ipython-input-1-47fd71e04f21>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-47fd71e04f21>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    ├── argparse (標準ライブラリ)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '├' (U+251C)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "run_class_finetuning.pyの実行時の関数呼び出し順序（木構造）"
      ],
      "metadata": {
        "id": "U-4XpUs2OGy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_class_finetuning.py\n",
        "└── main(args, ds_init)\n",
        "    ├── utils.init_distributed_mode(args)\n",
        "    ├── デバイスとシードの設定\n",
        "    ├── cudnn.benchmark = True\n",
        "    ├── dataset_train, dataset_test, dataset_val, ch_names, metrics = get_dataset(args)\n",
        "    │   └── prepare_TUAB_dataset または prepare_TUEV_dataset（utils.py内）\n",
        "    ├── サンプラーとデータローダーの設定\n",
        "    │   ├── torch.utils.data.DistributedSampler（トレーニング用）\n",
        "    │   ├── torch.utils.data.SequentialSampler（検証・テスト用）\n",
        "    │   └── torch.utils.data.DataLoader（データローダー作成）\n",
        "    ├── ロガーの設定（TensorboardLogger）\n",
        "    ├── model = get_models(args)\n",
        "    │   └── timm.models.create_model による。create_modelの中で args.modelが渡されてmodeling_finetune.py の中のモデル（labram_base_patch200_200）構造を取得する。\n",
        "    ├── モデルの初期化と読み込み   #モデルの重みがここで読まれる。\n",
        "    │   ├── 事前学習済みモデルのロード（args.finetune が指定されている場合）\n",
        "    │   ├── モデルの重みの調整（必要に応じて）\n",
        "    │   └── モデルをデバイスに転送（model.to(device)）\n",
        "    ├── モデルEMAの設定（必要に応じて）\n",
        "    ├── DistributedDataParallel の設定（分散学習の場合）\n",
        "    ├── オプティマイザとスケジューラの設定\n",
        "    │   ├── optimizer = create_optimizer(...)\n",
        "    │   ├── loss_scaler = NativeScaler()\n",
        "    │   ├── lr_schedule_values = utils.cosine_scheduler(...)\n",
        "    │   └── wd_schedule_values = utils.cosine_scheduler(...)\n",
        "    ├── 損失関数の設定（criterion）\n",
        "    ├── utils.auto_load_model(...)（チェックポイントの自動ロード）\n",
        "    ├── トレーニングループ開始（エポックごとに繰り返し）\n",
        "    │   ├── data_loader_train.sampler.set_epoch(epoch)\n",
        "    │   ├── train_stats = train_one_epoch(...)\n",
        "    │   │   ├── モデルをトレーニングモードに設定（model.train()）\n",
        "    │   │   ├── バッチごとのトレーニングループ\n",
        "    │   │   │   ├── 入力データとラベルを取得\n",
        "    │   │   │   ├── optimizer.zero_grad()\n",
        "    │   │   │   ├── 出力の計算（outputs = model(inputs)）\n",
        "    │   │   │   ├── 損失の計算（loss = criterion(outputs, targets)）\n",
        "    │   │   │   ├── loss_scaler(loss, optimizer, ...)\n",
        "    │   │   │   └── 学習率と重み減衰の更新\n",
        "    │   │   └── 統計情報の収集\n",
        "    │   ├── モデルの保存（必要に応じて）\n",
        "    │   ├── 検証（data_loader_val がある場合）\n",
        "    │   │   ├── val_stats = evaluate(...)\n",
        "    │   │   │   ├── モデルを評価モードに設定（model.eval()）\n",
        "    │   │   │   ├── バッチごとの評価ループ\n",
        "    │   │   │   │   ├── 入力データとラベルを取得\n",
        "    │   │   │   │   ├── 出力の計算（outputs = model(inputs)）\n",
        "    │   │   │   │   ├── 損失の計算（loss = criterion(outputs, targets)）\n",
        "    │   │   │   │   └── 統計情報の収集（metrics の計算）\n",
        "    │   │   └── 検証結果の表示とログ記録\n",
        "    │   ├── テスト（data_loader_test がある場合）\n",
        "    │   │   ├── test_stats = evaluate(...)\n",
        "    │   │   └── テスト結果の表示とログ記録\n",
        "    │   ├── 最良モデルの保存（必要に応じて）\n",
        "    │   └── エポックごとのログの保存\n",
        "    └── トレーニング時間の計測と表示\n"
      ],
      "metadata": {
        "id": "C_noGJx1OMVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "分類対象の数を変えたいときはモデルのヘッドを変えないといけない、まずモデル構造の変えかたを調べよう。"
      ],
      "metadata": {
        "id": "WsT0zzsshtKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ーーーモデル構造についてーーー\n",
        "\n",
        "modeling_finetune.pyの中でtimm の @register_model デコレーターを用いて、カスタムモデル(labram_base_patch200_200)が登録されています。\n",
        "\n",
        "args.model = 'labram_base_patch200_200'となってる影響で、\n",
        "\n",
        "get_model関数＝create_modelの引数をargs.model とすると。labram_base_patch200_200 構造をtimmのcreate_modelが作ってくれる仕組み。\n",
        "以下modeling_finetuneで独自モデルが登録されてる様子。\n",
        "\n",
        "（NeuralTransformerはLaBramの本体的クラス）"
      ],
      "metadata": {
        "id": "z11zrHPoTCKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@register_model\n",
        "def labram_base_patch200_200(pretrained=False, **kwargs):\n",
        "    model = NeuralTransformer(\n",
        "        patch_size=200, embed_dim=200, depth=12, num_heads=10, mlp_ratio=4,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "OYE17ZkkdP0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "ーーーーーーーーーーーーーー\n",
        "\n",
        "run_class_finetuning.pyの\n",
        "main関数内でモデルは以下のようにロードされる。\n",
        "以下のコードは、モデルの構造（modeling_finetune.py で定義された構造）に合わせてチェックポイントからロードした重みを調整する仕組みになっている。\n",
        "特に、ロードした重みの形状が現在のモデル構造と一致しない場合には、該当部分の重みを削除して適用されないようにしている点が特徴。\n",
        "\n",
        "つまり、元のLaBramの自己教師モデルの重みのヘッド部分を強制的に消して、クラス分類へ適用するようになってる。\n"
      ],
      "metadata": {
        "id": "j5QZgNobdPFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_models(args)  #モデル生成。 get_modelsはrun_class_finetuning中で定義\n",
        "\n",
        "    patch_size = model.patch_size\n",
        "    print(\"Patch size = %s\" % str(patch_size))\n",
        "    args.window_size = (1, args.input_size // patch_size)\n",
        "    args.patch_size = patch_size\n",
        "\n",
        "    if args.finetune:\n",
        "        if args.finetune.startswith('https'):\n",
        "            checkpoint = torch.hub.load_state_dict_from_url(\n",
        "                args.finetune, map_location='cpu', check_hash=True)\n",
        "        else:\n",
        "            checkpoint = torch.load(args.finetune, map_location='cpu')\n",
        "\n",
        "        print(\"Load ckpt from %s\" % args.finetune)\n",
        "        checkpoint_model = None\n",
        "        for model_key in args.model_key.split('|'):\n",
        "            if model_key in checkpoint:\n",
        "                checkpoint_model = checkpoint[model_key]\n",
        "                print(\"Load state_dict by model_key = %s\" % model_key)\n",
        "                break\n",
        "        if checkpoint_model is None:\n",
        "            checkpoint_model = checkpoint\n",
        "        if (checkpoint_model is not None) and (args.model_filter_name != ''):\n",
        "            all_keys = list(checkpoint_model.keys())\n",
        "            new_dict = OrderedDict()\n",
        "            for key in all_keys:\n",
        "                if key.startswith('student.'):\n",
        "                    new_dict[key[8:]] = checkpoint_model[key]\n",
        "                else:\n",
        "                    pass\n",
        "            checkpoint_model = new_dict\n",
        "\n",
        "        state_dict = model.state_dict()\n",
        "        for k in ['head.weight', 'head.bias']:\n",
        "            if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
        "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
        "                del checkpoint_model[k]\n",
        "\n",
        "        all_keys = list(checkpoint_model.keys())\n",
        "        for key in all_keys:\n",
        "            if \"relative_position_index\" in key:\n",
        "                checkpoint_model.pop(key)\n",
        "\n",
        "        utils.load_state_dict(model, checkpoint_model, prefix=args.model_prefix)\n"
      ],
      "metadata": {
        "id": "YtRAge0MTF_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以上から、私がもし分類数を変えたければ、modeling_finetune.py内の分類ヘッドを変えればいいことが判明した。"
      ],
      "metadata": {
        "id": "L0edGTHveCZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12/4 次に、今日は異なるデータセットに対して処理を行うために、データセットを処理する関数について調べよう。"
      ],
      "metadata": {
        "id": "FDCu_qnihnHU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zt55wGsliAUI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}